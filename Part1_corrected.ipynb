{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CqMyXJ3JTdEK"
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pACMBrcdUneY"
   },
   "source": [
    "Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dBWbdGlQu3tz"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#import os\n",
    "#os.chdir('/floyd/input/hgg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tables\n",
    "!pip install nibabel\n",
    "!pip install SimpleITK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rRs-V4SATf9f"
   },
   "source": [
    "Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 80
    },
    "colab_type": "code",
    "id": "dPvUii5gTRbE",
    "outputId": "558e3230-0b83-49b4-9033-88df1171ba0d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poudelas/anaconda3/envs/auto/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/poudelas/anaconda3/envs/auto/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/poudelas/anaconda3/envs/auto/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:521: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/poudelas/anaconda3/envs/auto/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:522: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/poudelas/anaconda3/envs/auto/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/poudelas/anaconda3/envs/auto/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage.io as io\n",
    "import skimage.color as color\n",
    "import keras\n",
    "from keras import backend as K\n",
    "import SimpleITK as sitk\n",
    "\n",
    "####\n",
    "import tables\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r-OKbXMzQgyz"
   },
   "outputs": [],
   "source": [
    "image_size = 240      # 240 pixels by 240 pixels\n",
    "smooth = 0.01         # used in loss, changed from .005"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "knzI-PaPT77Q"
   },
   "source": [
    "Specify the folder where images are located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.set_image_data_format('channels_first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(259, 155, 2, 240, 240)\n",
      "(259, 155, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "table_dir = '/home/poudelas/Documents/TF.Keras-Commonly-used-models-master/常用分割模型/Unet_family/table_data.hdf5'\n",
    "hdf5_file = tables.open_file(table_dir, mode='r+')\n",
    "data_storage    = hdf5_file.root.data\n",
    "truth_storage   = hdf5_file.root.truth\n",
    "brain_names     = hdf5_file.root.brain_names\n",
    "\n",
    "print(data_storage.shape)\n",
    "print(truth_storage.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load validation and test brains and save them as numpy (still faster than nii.gz)\n",
    "val_idx   = np.load('/home/poudelas/Documents/TF.Keras-Commonly-used-models-master/常用分割模型/Unet_family/val_idx.npy') # 27 brain indexes for validation\n",
    "test_idx  = np.load('/home/poudelas/Documents/TF.Keras-Commonly-used-models-master/常用分割模型/Unet_family/test_idx.npy') # 27 brain indexes for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGG_BraTS19_CBICA_BGN_1\n",
      "HGG_BraTS19_TCIA01_390_1\n",
      "HGG_BraTS19_CBICA_AVV_1\n",
      "HGG_BraTS19_CBICA_BAP_1\n",
      "HGG_BraTS19_TCIA03_265_1\n",
      "HGG_BraTS19_TCIA03_121_1\n",
      "HGG_BraTS19_TCIA03_296_1\n",
      "HGG_BraTS19_TCIA04_437_1\n",
      "HGG_BraTS19_CBICA_AQY_1\n",
      "HGG_BraTS19_TCIA08_278_1\n",
      "HGG_BraTS19_CBICA_AXQ_1\n",
      "HGG_BraTS19_TCIA01_335_1\n",
      "HGG_BraTS19_TCIA02_179_1\n",
      "HGG_BraTS19_2013_26_1\n",
      "HGG_BraTS19_2013_13_1\n",
      "HGG_BraTS19_CBICA_BJY_1\n",
      "HGG_BraTS19_TCIA04_328_1\n",
      "HGG_BraTS19_TCIA03_498_1\n",
      "HGG_BraTS19_TCIA01_460_1\n",
      "HGG_BraTS19_TCIA06_211_1\n",
      "HGG_BraTS19_TCIA04_149_1\n",
      "HGG_BraTS19_TCIA08_162_1\n",
      "HGG_BraTS19_CBICA_AWV_1\n",
      "HGG_BraTS19_TCIA03_199_1\n",
      "HGG_BraTS19_CBICA_AWG_1\n",
      "HGG_BraTS19_CBICA_ANI_1\n"
     ]
    }
   ],
   "source": [
    "# We will use these numpys for validation and test generator\n",
    "validation_save_dir = '/home/poudelas/PycharmProjects/efficientnet/val_save/'\n",
    "for idx_val in val_idx:\n",
    "    #print(data_storage[idx_val].shape)\n",
    "    #print(truth_storage[idx_val].shape)                   \n",
    "    name = brain_names[idx_val].decode(\"utf-8\")\n",
    "    print(name)\n",
    "    folder = os.path.join(validation_save_dir, name)\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "    np.save( os.path.join(validation_save_dir, folder, name + '_channels') , data_storage[idx_val])\n",
    "    np.save( os.path.join(validation_save_dir, folder, name + '_label') , truth_storage[idx_val])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HGG_BraTS19_2013_10_1\n",
      "HGG_BraTS19_TCIA05_444_1\n",
      "HGG_BraTS19_2013_22_1\n",
      "HGG_BraTS19_CBICA_AUN_1\n",
      "HGG_BraTS19_TCIA08_242_1\n",
      "HGG_BraTS19_CBICA_ASW_1\n",
      "HGG_BraTS19_TCIA03_419_1\n",
      "HGG_BraTS19_TCIA08_436_1\n",
      "HGG_BraTS19_CBICA_AYG_1\n",
      "HGG_BraTS19_2013_11_1\n",
      "HGG_BraTS19_CBICA_ABN_1\n",
      "HGG_BraTS19_CBICA_AOS_1\n",
      "HGG_BraTS19_CBICA_ASG_1\n",
      "HGG_BraTS19_TCIA06_372_1\n",
      "HGG_BraTS19_TCIA01_180_1\n",
      "HGG_BraTS19_CBICA_APY_1\n",
      "HGG_BraTS19_CBICA_AOC_1\n",
      "HGG_BraTS19_CBICA_AYA_1\n",
      "HGG_BraTS19_CBICA_BLJ_1\n",
      "HGG_BraTS19_CBICA_AQD_1\n",
      "HGG_BraTS19_TCIA01_190_1\n",
      "HGG_BraTS19_CBICA_APK_1\n",
      "HGG_BraTS19_CBICA_AOH_1\n",
      "HGG_BraTS19_CBICA_AQV_1\n",
      "HGG_BraTS19_TMC_11964_1\n",
      "HGG_BraTS19_CBICA_AUR_1\n"
     ]
    }
   ],
   "source": [
    "test_save_dir = '/home/poudelas/PycharmProjects/efficientnet/test_np/'\n",
    "for idx_test in test_idx:\n",
    "    #print(data_storage[idx_test].shape)\n",
    "    #print(truth_storage[idx_test].shape)                   \n",
    "    name = brain_names[idx_test].decode(\"utf-8\")\n",
    "    print(name)\n",
    "    folder = os.path.join(test_save_dir, name)\n",
    "    if not os.path.isdir(folder):\n",
    "        os.mkdir(folder)\n",
    "    np.save( os.path.join(test_save_dir, folder, name + '_channels') , data_storage[idx_test])\n",
    "    np.save( os.path.join(test_save_dir, folder, name + '_label') , truth_storage[idx_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# closing the table\n",
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G73ur1tVerBO"
   },
   "source": [
    "## Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDAiqr1ClNEE"
   },
   "source": [
    "Define Dice coefficient loss for segmentation. Let s use standard versions of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yvyq9Gf9lNrE"
   },
   "outputs": [],
   "source": [
    "#source: https://www.kaggle.com/c/severstal-steel-defect-detection/discussion/107182\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = K.sum(y_true_f * y_pred_f)\n",
    "    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "\n",
    "def dice_coef_loss(y_true, y_pred):\n",
    "    return 1-dice_coef(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "faJUxDpCkb6U"
   },
   "source": [
    "Define uNet model for full tumour segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MzSlHtakkynJ"
   },
   "outputs": [],
   "source": [
    "def unet_model():\n",
    "    inputs = keras.layers.Input((2, image_size, image_size)) #using 2 channels, one will be T2 and Flair later\n",
    "    \n",
    "    conv1 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same') (inputs)\n",
    "    batch1 = keras.layers.BatchNormalization(axis=1)(conv1)\n",
    "    conv1 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same') (batch1)\n",
    "    batch1 = keras.layers.BatchNormalization(axis=1)(conv1)\n",
    "    pool1 = keras.layers.MaxPooling2D((2, 2)) (batch1)\n",
    "    \n",
    "    conv2 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same') (pool1)\n",
    "    batch2 = keras.layers.BatchNormalization(axis=1)(conv2)\n",
    "    conv2 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same') (batch2)\n",
    "    batch2 = keras.layers.BatchNormalization(axis=1)(conv2)\n",
    "    pool2 = keras.layers.MaxPooling2D((2, 2)) (batch2)\n",
    "    \n",
    "    conv3 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same') (pool2)\n",
    "    batch3 = keras.layers.BatchNormalization(axis=1)(conv3)\n",
    "    conv3 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same') (batch3)\n",
    "    batch3 = keras.layers.BatchNormalization(axis=1)(conv3)\n",
    "    pool3 = keras.layers.MaxPooling2D((2, 2)) (batch3)\n",
    "    \n",
    "    conv4 = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same') (pool3)\n",
    "    batch4 = keras.layers.BatchNormalization(axis=1)(conv4)\n",
    "    conv4 = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same') (batch4)\n",
    "    batch4 = keras.layers.BatchNormalization(axis=1)(conv4)\n",
    "    pool4 = keras.layers.MaxPooling2D(pool_size=(2, 2)) (batch4)\n",
    "    \n",
    "    conv5 = keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same') (pool4)\n",
    "    batch5 = keras.layers.BatchNormalization(axis=1)(conv5)\n",
    "    conv5 = keras.layers.Conv2D(1024, (3, 3), activation='relu', padding='same') (batch5)\n",
    "    batch5 = keras.layers.BatchNormalization(axis=1)(conv5)\n",
    "    \n",
    "    up6 = keras.layers.Conv2DTranspose(512, (2, 2), strides=(2, 2), padding='same') (batch5)\n",
    "    up6 = keras.layers.concatenate([up6, conv4], axis=1)\n",
    "    conv6 = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same') (up6)\n",
    "    batch6 = keras.layers.BatchNormalization(axis=1)(conv6)\n",
    "    conv6 = keras.layers.Conv2D(512, (3, 3), activation='relu', padding='same') (batch6)\n",
    "    batch6 = keras.layers.BatchNormalization(axis=1)(conv6)\n",
    "    \n",
    "    up7 = keras.layers.Conv2DTranspose(256, (2, 2), strides=(2, 2), padding='same') (batch6)\n",
    "    up7 = keras.layers.concatenate([up7, conv3], axis=1)\n",
    "    conv7 =keras.layers. Conv2D(256, (3, 3), activation='relu', padding='same') (up7)\n",
    "    batch7 = keras.layers.BatchNormalization(axis=1)(conv7)\n",
    "    conv7 = keras.layers.Conv2D(256, (3, 3), activation='relu', padding='same') (batch7)\n",
    "    batch7 = keras.layers.BatchNormalization(axis=1)(conv7)\n",
    "    \n",
    "    up8 = keras.layers.Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same') (batch7)\n",
    "    up8 = keras.layers.concatenate([up8, conv2], axis=1)\n",
    "    conv8 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same') (up8)\n",
    "    batch8 = keras.layers.BatchNormalization(axis=1)(conv8)\n",
    "    conv8 = keras.layers.Conv2D(128, (3, 3), activation='relu', padding='same') (batch8)\n",
    "    batch8 = keras.layers.BatchNormalization(axis=1)(conv8)\n",
    "    \n",
    "    up9 = keras.layers.Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same') (batch8)\n",
    "    up9 = keras.layers.concatenate([up9, conv1], axis=1)\n",
    "    conv9 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same') (up9)\n",
    "    batch9 = keras.layers.BatchNormalization(axis=1)(conv9)\n",
    "    conv9 = keras.layers.Conv2D(64, (3, 3), activation='relu', padding='same') (batch9)\n",
    "    batch9 = keras.layers.BatchNormalization(axis=1)(conv9)\n",
    "\n",
    "    conv10 = keras.layers.Conv2D(1, (1, 1), activation='sigmoid')(batch9) # sigmoid crunching number [0,1] for predicted probability of segment\n",
    "                                                                          # close to 1 means tumor\n",
    "    model = keras.models.Model(inputs=[inputs], outputs=[conv10])\n",
    "\n",
    "    model.compile(optimizer=keras.optimizers.Adam(lr=1e-6), loss=dice_coef_loss, metrics=[dice_coef])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/poudelas/anaconda3/envs/auto/lib/python3.6/site-packages/keras_applications/imagenet_utils.py:272: UserWarning: This model usually expects 1 or 3 input channels. However, it was passed an input_shape with 224 input channels.\n",
      "  str(input_shape[0]) + ' input channels.')\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input size must be at least 32x32; got `input_shape=(224, 224, 2)`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-17288afcc53c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLeakyReLU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAdd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mMaxPool2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mUpSampling2D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mConv2DTranspose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mBatchNormalization\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mbase_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mefn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEfficientNetB0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/efficientnet/efficientnet/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'models'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'utils'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/efficientnet/efficientnet/model.py\u001b[0m in \u001b[0;36mEfficientNetB0\u001b[0;34m(include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0minput_tensor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0mpooling\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpooling\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclasses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m     )\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/efficientnet/efficientnet/model.py\u001b[0m in \u001b[0;36mEfficientNet\u001b[0;34m(width_coefficient, depth_coefficient, default_resolution, dropout_rate, drop_connect_rate, depth_divisor, blocks_args, model_name, include_top, weights, input_tensor, input_shape, pooling, classes, **kwargs)\u001b[0m\n\u001b[1;32m    324\u001b[0m                                       \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_data_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m                                       \u001b[0mrequire_flatten\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minclude_top\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                                       weights=weights)\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput_tensor\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/auto/lib/python3.6/site-packages/keras_applications/imagenet_utils.py\u001b[0m in \u001b[0;36m_obtain_input_shape\u001b[0;34m(input_shape, default_size, min_size, data_format, require_flatten, weights)\u001b[0m\n\u001b[1;32m    306\u001b[0m                                      \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'x'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin_size\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m                                      \u001b[0;34m'; got `input_shape='\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m                                      str(input_shape) + '`')\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0minput_shape\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input size must be at least 32x32; got `input_shape=(224, 224, 2)`"
     ]
    }
   ],
   "source": [
    "# import efficientnet.keras as efn\n",
    "# from keras.models import Model\n",
    "# from keras.layers.convolutional import Conv2D\n",
    "# from keras.layers import LeakyReLU, Add, Input,MaxPool2D,UpSampling2D,concatenate,Conv2DTranspose,BatchNormalization,Dropout\n",
    "# base_model = efn.EfficientNetB0(weights=None,include_top=False,input_shape=(224,224,2))\n",
    "\n",
    "\n",
    "# input_model = base_model.input\n",
    "# start_neurons = 8\n",
    "# dropout_ratio = 0.1\n",
    "\n",
    "# def convolution_block(x, filters, size, strides=(1,1), padding='same', activation=True):\n",
    "#     x = Conv2D(filters, size, strides=strides, padding=padding)(x)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     if activation == True:\n",
    "#         x = LeakyReLU(alpha=0.1)(x)\n",
    "#     return x\n",
    "\n",
    "# def residual_block(blockInput, num_filters=16):\n",
    "#     x = LeakyReLU(alpha=0.1)(blockInput)\n",
    "#     x = BatchNormalization()(x)\n",
    "#     blockInput = BatchNormalization()(blockInput)\n",
    "#     x = convolution_block(x, num_filters, (3,3) )\n",
    "#     x = convolution_block(x, num_filters, (3,3), activation=False)\n",
    "#     x = Add()([x, blockInput])\n",
    "#     return x\n",
    "\n",
    "\n",
    "# conv5 = base_model.get_layer('top_activation').output\n",
    "# conv4 = base_model.get_layer('block6a_expand_activation').output\n",
    "# conv3 = base_model.get_layer('block4a_expand_activation').output\n",
    "# conv2 = base_model.get_layer('block3a_expand_activation').output\n",
    "# conv1 = base_model.get_layer('block2a_expand_activation').output\n",
    "\n",
    "# #Middle 7*7\n",
    "# convm = Conv2D(start_neurons * 32, (3, 3), activation=None, padding=\"same\",name='conv_middle')(conv5)\n",
    "# convm = residual_block(convm,start_neurons * 32)\n",
    "# convm = residual_block(convm,start_neurons * 32)\n",
    "# convm = LeakyReLU(alpha=0.1)(convm) #7*7\n",
    "\n",
    "# #14*14\n",
    "# deconv4 = Conv2DTranspose(start_neurons*16,(3,3),strides=(2,2),padding=\"same\")(convm)\n",
    "# uconv4 = concatenate([deconv4,conv4])\n",
    "# uconv4 = Dropout(dropout_ratio)(uconv4)\n",
    "# uconv4 = Conv2D(start_neurons * 16, (3, 3), activation=None, padding=\"same\")(uconv4)\n",
    "# uconv4 = residual_block(uconv4,start_neurons * 16)\n",
    "# uconv4 = LeakyReLU(alpha=0.1)(uconv4)\n",
    "\n",
    "# #28*28\n",
    "# deconv3 = Conv2DTranspose(start_neurons*8,(3,3),strides=(2,2),padding=\"same\")(uconv4)\n",
    "# uconv3 = concatenate([deconv3,conv3])\n",
    "# uconv3 = Dropout(dropout_ratio)(uconv3)\n",
    "# uconv3 = Conv2D(start_neurons * 8, (3, 3), activation=None, padding=\"same\")(uconv3)\n",
    "# uconv3 = residual_block(uconv3,start_neurons * 8)\n",
    "# uconv3 = LeakyReLU(alpha=0.1)(uconv3)\n",
    "\n",
    "# #56*56\n",
    "# deconv2 = Conv2DTranspose(start_neurons*4,(3,3),strides=(2,2),padding=\"same\")(uconv3)\n",
    "# uconv2 = concatenate([deconv2,conv2])\n",
    "# uconv2 = Dropout(dropout_ratio)(uconv2)\n",
    "# uconv2 = Conv2D(start_neurons * 4, (3, 3), activation=None, padding=\"same\")(uconv2)\n",
    "# uconv2 = residual_block(uconv2,start_neurons * 4)\n",
    "# uconv2 = LeakyReLU(alpha=0.1)(uconv2)\n",
    "\n",
    "# #112*112\n",
    "# deconv1 = Conv2DTranspose(start_neurons*2,(3,3),strides=(2,2),padding=\"same\")(uconv2)\n",
    "# uconv1 = concatenate([deconv1,conv1])\n",
    "# uconv1 = Dropout(dropout_ratio)(uconv1)\n",
    "# uconv1 = Conv2D(start_neurons * 2, (3, 3), activation=None, padding=\"same\")(uconv1)\n",
    "# uconv1 = residual_block(uconv1,start_neurons * 2)\n",
    "# uconv1 = LeakyReLU(alpha=0.1)(uconv1)\n",
    "\n",
    "# #224*224\n",
    "# deconv0 = Conv2DTranspose(start_neurons*1,(3,3),strides=(2,2),padding=\"same\")(uconv1)\n",
    "# uconv0 = Dropout(dropout_ratio)(deconv0)\n",
    "# uconv0 = Conv2D(start_neurons * 1, (3, 3), activation=None, padding=\"same\")(uconv0)\n",
    "# uconv0 = residual_block(uconv0,start_neurons * 1)\n",
    "# uconv0 = LeakyReLU(alpha=0.1)(uconv0)\n",
    "\n",
    "# output_layer = Conv2D(1, (1,1), padding=\"same\", activation=\"sigmoid\")(uconv0)\n",
    "\n",
    "# model = Model(input_model,[output_layer])\n",
    "# print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rb02PFwWlE3r"
   },
   "outputs": [],
   "source": [
    "model_full = unet_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eaBS9fz5P7UT"
   },
   "source": [
    "## Data Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y4-WLGg8QyDt"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Utilities for real-time multi-thread data generator\n",
    "\"\"\"\n",
    "\n",
    "import scipy\n",
    "import numpy as np\n",
    "from keras.utils import Sequence, to_categorical\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "class CustomDataGenerator(Sequence):\n",
    "    \n",
    "    def __init__(self, hdf5_file, brain_idx, batch_size=16, mode='train', horizontal_flip=False,\n",
    "                 vertical_flip=False, rotation_range=0, zoom_range=0., shuffle=True):\n",
    "        \"\"\"\n",
    "        Custom data generator based on Keras Sequance class.\n",
    "        This implementation enables multiprocessing and on-the-fly data augmentation \n",
    "        which will speed up training, especially in the task of brain tumor segmentation\n",
    "        that suffers from time-consuming data processing.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        hdf5_file : file.File\n",
    "            An opend hdf5 file that contains all data.\n",
    "        brain_idx : array\n",
    "            The brain indexes corresponing to a specific fold. All of these \n",
    "            brain indexes will be use for training and the ones which are \n",
    "            not in 'brain_idx' will be used for validation\n",
    "        batch_size : int\n",
    "            The number of input/output arrays that will be generated each \n",
    "            time. The default is 16.\n",
    "        view : str\n",
    "            'axial', 'sagittal' or 'coronal'. The generator will extract\n",
    "            2D slices and perform normalization with respect to the chosen view.\n",
    "            The defualt is axial.\n",
    "        mode : str\n",
    "            Prepare the DataGenerator for 'train' or 'validation' phase. \n",
    "            The default is 'train'.\n",
    "        horizontal_flip : bool\n",
    "            Whether to use horizontal flip for data augmentation. The default is False.\n",
    "        vertical_flip : bool\n",
    "            Whether to use vertical flip for data augmentation. The default is False.\n",
    "        rotation_range : float\n",
    "            Random rotation for data augmentation. The default is 0.\n",
    "        zoom_range : float\n",
    "            Random zoom for data augmentation. The default is 0.\n",
    "        shuffle : bool\n",
    "            Whether to shuffle data. The default is True. Note that if mode='validation' \n",
    "            it will not shufflw tha data.\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_storage    = hdf5_file.root.data\n",
    "        self.truth_storage   = hdf5_file.root.truth   \n",
    "        \n",
    "        total_brains         = self.data_storage.shape[0]\n",
    "        self.brain_idx       = brain_idx\n",
    "        self.batch_size      = batch_size\n",
    "        \n",
    "        '''\n",
    "        if view == 'axial':\n",
    "            self.view_axes = (0, 1, 2, 3)            \n",
    "        elif view == 'sagittal': \n",
    "            self.view_axes = (2, 1, 0, 3)\n",
    "        elif view == 'coronal':\n",
    "            self.view_axes = (1, 2, 0, 3)            \n",
    "        else:\n",
    "            ValueError('unknown input view => {}'.format(view))\n",
    "        '''  \n",
    "        \n",
    "        self.mode            = mode\n",
    "        self.horizontal_flip = horizontal_flip\n",
    "        self.vertical_flip   = vertical_flip\n",
    "        self.rotation_range  = rotation_range       \n",
    "        self.zoom_range      = [1 - zoom_range, 1 + zoom_range]\n",
    "        self.shuffle         = shuffle\n",
    "        self.data_shape      = tuple(np.array(self.data_storage.shape[1:]))\n",
    "        \n",
    "        print('model: {}'.format(mode))\n",
    "        print('Using {} out of {} brains'.format(len(self.brain_idx), total_brains), end=' ')\n",
    "        print('({} out of {} 2D slices)'.format(len(self.brain_idx) * self.data_shape[0], total_brains * self.data_shape[0]))\n",
    "        print('-----'*10)\n",
    "        \n",
    "\n",
    "        self.on_epoch_end()   \n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(np.floor( len(self.indexes) / self.batch_size))\n",
    "    \n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # Generate indexes of the batch\n",
    "        idx = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        # Generate data\n",
    "        X_batch, Y_batch = self.data_load_and_preprocess(idx)\n",
    "\n",
    "        return X_batch, Y_batch\n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Updates indexes after each epoch\n",
    "        \"\"\"\n",
    "        tmp=[]\n",
    "        for i in self.brain_idx:\n",
    "            for j in range(self.data_shape[0]):\n",
    "                tmp.append((i,j))\n",
    "        self.indexes = tmp\n",
    "            \n",
    "        if self.mode=='train' and self.shuffle:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "            \n",
    "    def data_load_and_preprocess(self, idx):\n",
    "        \"\"\"\n",
    "        Generates data containing batch_size samples\n",
    "        \"\"\"\n",
    "        slice_batch = []\n",
    "        label_batch = []\n",
    "\n",
    "        # Generate data\n",
    "        for i in idx:\n",
    "            brain_number     = i[0]\n",
    "            slice_number     = i[1]\n",
    "            slice_, label_   = self.read_data(brain_number, slice_number)\n",
    "            #slice_           = self.normalize_modalities(slice_)\n",
    "            slice_and_label  = np.concatenate((slice_, label_) , axis=0)\n",
    "            params           = self.get_random_transform()\n",
    "            slice_and_label  = self.apply_transform(slice_and_label, params)\n",
    "            slice_           = slice_and_label[:2]\n",
    "            label_           = slice_and_label[2:]\n",
    "            #label_           = to_categorical(label_, 4) \n",
    "            \n",
    "            slice_batch.append(slice_)\n",
    "            label_batch.append(label_)\n",
    "            \n",
    "        return np.array(slice_batch), np.array(label_batch)\n",
    "    \n",
    "    \n",
    "    \n",
    "    def read_data(self, brain_number, slice_number):\n",
    "        \n",
    "        \"\"\"\n",
    "        Reads data from table with respect to the 'view'\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        slice_    = self.data_storage[brain_number][slice_number]\n",
    "        label_    = self.truth_storage[brain_number][slice_number]\n",
    "        label_    = np.expand_dims(label_, axis=0)\n",
    "        \n",
    "        return slice_, label_ \n",
    "        \n",
    "    \n",
    "    def normalize_slice(self, slice):\n",
    "        \n",
    "        \"\"\"\n",
    "        Removes 1% of the top and bottom intensities and perform\n",
    "        normalization on the input 2D slice.\n",
    "        \"\"\"\n",
    "        b = np.percentile(slice, 99)\n",
    "        t = np.percentile(slice, 1)\n",
    "        slice = np.clip(slice, t, b)\n",
    "        if np.std(slice)==0:\n",
    "            return slice\n",
    "        else:\n",
    "            slice = (slice - np.mean(slice)) / np.std(slice)\n",
    "            return slice\n",
    "        \n",
    "        \n",
    "    def normalize_modalities(self, Slice): \n",
    "        \n",
    "        \"\"\"\n",
    "        Performs normalization on each modalities of input\n",
    "        \"\"\"\n",
    "\n",
    "        normalized_slices = np.zeros_like(Slice).astype(np.float32)\n",
    "        for slice_ix in range(4):\n",
    "            normalized_slices[..., slice_ix] = self.normalize_slice(Slice[..., slice_ix])\n",
    "    \n",
    "        return normalized_slices  \n",
    "    \n",
    "\n",
    "    def flip_axis(self, x, axis):\n",
    "        \n",
    "        x = np.asarray(x).swapaxes(axis, 0)\n",
    "        x = x[::-1, ...]\n",
    "        x = x.swapaxes(0, axis)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "    def apply_transform(self, x, transform_parameters):\n",
    "        \n",
    "        x = apply_affine_transform(x, transform_parameters.get('theta', 0),\n",
    "                           transform_parameters.get('tx', 0),\n",
    "                           transform_parameters.get('ty', 0),\n",
    "                           transform_parameters.get('shear', 0),\n",
    "                           transform_parameters.get('zx', 1),\n",
    "                           transform_parameters.get('zy', 1),\n",
    "                           row_axis=0,\n",
    "                           col_axis=1,\n",
    "                           channel_axis=0)\n",
    "        if transform_parameters.get('flip_horizontal', False):\n",
    "            x = self.flip_axis(x, 2)\n",
    "        if transform_parameters.get('flip_vertical', False):\n",
    "            x = self.flip_axis(x, 1)            \n",
    "        return x\n",
    "        \n",
    "    def get_random_transform(self):\n",
    "    \n",
    "        if self.rotation_range:\n",
    "            theta = np.random.uniform(-self.rotation_range,self.rotation_range)    \n",
    "        else:\n",
    "            theta = 0            \n",
    " \n",
    "        if self.zoom_range[0] == 1 and self.zoom_range[1] == 1:\n",
    "            zx, zy = 1, 1\n",
    "        else:\n",
    "            zx, zy = np.random.uniform(self.zoom_range[0],self.zoom_range[1], 2)\n",
    "            \n",
    "        flip_horizontal = (np.random.random() < 0.5) * self.horizontal_flip    \n",
    "        flip_vertical   = (np.random.random() < 0.5) * self.vertical_flip\n",
    "        \n",
    "        transform_parameters = {'flip_horizontal': flip_horizontal,\n",
    "                                'flip_vertical':flip_vertical,\n",
    "                                'theta': theta, \n",
    "                                'zx': zx, \n",
    "                                'zy': zy}\n",
    "    \n",
    "        return transform_parameters        \n",
    "        \n",
    "\"\"\"\n",
    "The two following functions are from ImageDataGenerator class of keras.\n",
    "https://github.com/keras-team/keras/blob/master/keras/preprocessing/image.py\n",
    "\"\"\"\n",
    "    \n",
    "def apply_affine_transform(x, theta=0, tx=0, ty=0, shear=0, zx=1, zy=1,\n",
    "                           row_axis=0, col_axis=1, channel_axis=2,\n",
    "                           fill_mode='nearest', cval=0.):\n",
    "    \"\"\"Applies an affine transformation specified by the parameters given.\n",
    "\n",
    "    # Arguments\n",
    "        x: 2D numpy array, single image.\n",
    "        theta: Rotation angle in degrees.\n",
    "        tx: Width shift.\n",
    "        ty: Heigh shift.\n",
    "        shear: Shear angle in degrees.\n",
    "        zx: Zoom in x direction.\n",
    "        zy: Zoom in y direction\n",
    "        row_axis: Index of axis for rows in the input image.\n",
    "        col_axis: Index of axis for columns in the input image.\n",
    "        channel_axis: Index of axis for channels in the input image.\n",
    "        fill_mode: Points outside the boundaries of the input\n",
    "            are filled according to the given mode\n",
    "            (one of `{'constant', 'nearest', 'reflect', 'wrap'}`).\n",
    "        cval: Value used for points outside the boundaries\n",
    "            of the input if `mode='constant'`.\n",
    "\n",
    "    # Returns\n",
    "        The transformed version of the input.\n",
    "    \"\"\"\n",
    "    transform_matrix = None\n",
    "    if theta != 0:\n",
    "        theta = np.deg2rad(theta)\n",
    "        rotation_matrix = np.array([[np.cos(theta), -np.sin(theta), 0],\n",
    "                                    [np.sin(theta), np.cos(theta), 0],\n",
    "                                    [0, 0, 1]])\n",
    "        transform_matrix = rotation_matrix\n",
    "\n",
    "    if tx != 0 or ty != 0:\n",
    "        shift_matrix = np.array([[1, 0, tx],\n",
    "                                 [0, 1, ty],\n",
    "                                 [0, 0, 1]])\n",
    "        if transform_matrix is None:\n",
    "            transform_matrix = shift_matrix\n",
    "        else:\n",
    "            transform_matrix = np.dot(transform_matrix, shift_matrix)\n",
    "\n",
    "    if shear != 0:\n",
    "        shear = np.deg2rad(shear)\n",
    "        shear_matrix = np.array([[1, -np.sin(shear), 0],\n",
    "                                 [0, np.cos(shear), 0],\n",
    "                                 [0, 0, 1]])\n",
    "        if transform_matrix is None:\n",
    "            transform_matrix = shear_matrix\n",
    "        else:\n",
    "            transform_matrix = np.dot(transform_matrix, shear_matrix)\n",
    "\n",
    "    if zx != 1 or zy != 1:\n",
    "        zoom_matrix = np.array([[zx, 0, 0],\n",
    "                                [0, zy, 0],\n",
    "                                [0, 0, 1]])\n",
    "        if transform_matrix is None:\n",
    "            transform_matrix = zoom_matrix\n",
    "        else:\n",
    "            transform_matrix = np.dot(transform_matrix, zoom_matrix)\n",
    "\n",
    "    if transform_matrix is not None:\n",
    "        h, w = x.shape[row_axis], x.shape[col_axis]\n",
    "        transform_matrix = transform_matrix_offset_center(\n",
    "            transform_matrix, h, w)\n",
    "        x = np.rollaxis(x, channel_axis, 0)\n",
    "        final_affine_matrix = transform_matrix[:2, :2]\n",
    "        final_offset = transform_matrix[:2, 2]\n",
    "\n",
    "        channel_images = [scipy.ndimage.interpolation.affine_transform(\n",
    "            x_channel,\n",
    "            final_affine_matrix,\n",
    "            final_offset,\n",
    "            order=1,\n",
    "            mode=fill_mode,\n",
    "            cval=cval) for x_channel in x]\n",
    "        x = np.stack(channel_images, axis=0)\n",
    "        x = np.rollaxis(x, 0, channel_axis + 1)\n",
    "    return x\n",
    "\n",
    "def transform_matrix_offset_center(matrix, x, y):\n",
    "    o_x = float(x) / 2 + 0.5\n",
    "    o_y = float(y) / 2 + 0.5\n",
    "    offset_matrix = np.array([[1, 0, o_x], [0, 1, o_y], [0, 0, 1]])\n",
    "    reset_matrix = np.array([[1, 0, -o_x], [0, 1, -o_y], [0, 0, 1]])\n",
    "    transform_matrix = np.dot(np.dot(offset_matrix, matrix), reset_matrix)\n",
    "    return transform_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "\n",
    "def imgDataGen_full(data_dir, batch_size = 20):\n",
    "    i = 0\n",
    "    slc = 0  \n",
    "    all_brain_dirs = glob(data_dir+'/*') # * means everything in that directory\n",
    "    i_max = len(all_brain_dirs)-1\n",
    "    #takes care of iterating the entire batch\n",
    "    brain_channels = np.load(glob(os.path.join(all_brain_dirs[i],'*_channels*'))[0])\n",
    "    Seg_full       = np.load(glob(os.path.join(all_brain_dirs[i],'*_label*'))[0])\n",
    "    while True:    \n",
    "        if slc+batch_size>154:\n",
    "            i += 1\n",
    "            slc=0\n",
    "            if i>i_max:\n",
    "                i=0\n",
    "            brain_channels = np.load(glob(os.path.join(all_brain_dirs[i],'*_channels*'))[0])\n",
    "            Seg_full       = np.load(glob(os.path.join(all_brain_dirs[i],'*_label*'))[0])\n",
    "        \n",
    "        #Pulling in the data and calling read_data function to numpy arrays                               \n",
    "        #the convolutional layer expects data in this format\n",
    "        #(batch_size, channels, img_height, img_width)\n",
    "        img_train = np.zeros((batch_size,2,240,240),np.float32) #creates an array in required dimensions\n",
    "        img_seg   = np.zeros((batch_size,1,240,240),np.float32) #img segment is ground truth/mask/label, creates an array in required dimensions\n",
    "\n",
    "        for m in range(batch_size):                           #the training image includes pixels from Img_Flair and Img_T2\n",
    "            img_train[m,:,:,:] = brain_channels[slc+m:slc+m+1,:,:,:] #the first channel of the image has data from Img_Flair, 0 corresponds to first channel\n",
    "            img_seg [m,0,:,:]  = Seg_full[slc+m:slc+m+1,:,:]  #now preparing training label, Seg_full contains whole tumor mask details, single channel 0\n",
    "        slc += batch_size                                     #takes care of iterating entire batch\n",
    "        yield [img_train, img_seg]                            #returns image and ground truth for this batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 2, 240, 240)\n",
      "(20, 1, 240, 240)\n"
     ]
    }
   ],
   "source": [
    "# testing generator \n",
    "\n",
    "val_dir = '/home/poudelas/PycharmProjects/efficientnet/val_save/'\n",
    "val_gen = imgDataGen_full(val_dir, batch_size = 20)\n",
    "x_batch, y_batch = next(val_gen)\n",
    "print(x_batch.shape)\n",
    "print(y_batch.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b-sifTU2yHN0"
   },
   "source": [
    "## Train and Evaluate the model on images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1fXL2-QDy6d3"
   },
   "source": [
    "Train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf5_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_dir1 = '/home/poudelas/Documents/TF.Keras-Commonly-used-models-master/常用分割模型/Unet_family/table_data.hdf5'\n",
    "hdf5_file1 = tables.open_file(table_dir1, mode='r+')\n",
    "\n",
    "#table_dir2 = './table_data2.hdf5'\n",
    "#hdf5_file2 = tables.open_file(table_dir2, mode='r+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QxWQq0l5WsFg"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model: train\n",
      "Using 207 out of 259 brains (32085 out of 40145 2D slices)\n",
      "--------------------------------------------------\n",
      "Epoch 1/10\n",
      "1604/1604 [==============================] - 1516s 945ms/step - loss: 0.9259 - dice_coef: 0.0741 - val_loss: 0.9332 - val_dice_coef: 0.0668\n",
      "Epoch 2/10\n",
      "1004/1604 [=================>............] - ETA: 8:32 - loss: 0.9032 - dice_coef: 0.0968"
     ]
    }
   ],
   "source": [
    "n_epochs = 10\n",
    "\n",
    "train_idx = np.load('/home/poudelas/Documents/TF.Keras-Commonly-used-models-master/常用分割模型/Unet_family/train_idx.npy') # 207 brain indexes for training\n",
    "#val_idx   = np.load('val_idx.npy') # 27 brain indexes for validation\n",
    "#test_idx  = np.load('test_idx.npy') # 27 brain indexes for testing\n",
    "val_dir = '/home/poudelas/PycharmProjects/efficientnet/val_save/'\n",
    "test_dir = '/home/poudelas/PycharmProjects/efficientnet/test_np/'\n",
    "\n",
    "train_gen = CustomDataGenerator(hdf5_file1, train_idx, batch_size=20, mode='train', \n",
    "                                horizontal_flip=True, vertical_flip=True, shuffle=True)\n",
    "\n",
    "val_gen =  imgDataGen_full(val_dir, batch_size = 20)\n",
    "test_gen =  imgDataGen_full(test_dir, batch_size = 20)\n",
    "# there is no need to set steps_per_epoch, When we use a sequence generator, len(generator) will be considered as steps.\n",
    "#history = model_full.fit_generator(train_gen, epochs=n_epochs, use_multiprocessing=False)\n",
    "\n",
    "# validation_steps = #brains * #slices / batch_size\n",
    "history = model_full.fit_generator(train_gen, epochs=n_epochs, use_multiprocessing=False, validation_data = val_gen, validation_steps=int((27 * 155)/10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full.save('mark1.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Il7axYghuu9Z"
   },
   "source": [
    "Plot training & validation accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RwPkakv1uu9a"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training & validation learning curves\n",
    "plt.plot(history.history['dice_coef'])\n",
    "plt.plot(history.history['val_dice_coef'])\n",
    "plt.title('Model Dice Score')\n",
    "plt.ylabel('Dice Score')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SOyHKYfEy3B"
   },
   "source": [
    "Mean Dice During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ckJudtOfxG6A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7406793"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(history.history['dice_coef'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n8GafHS0E4oX"
   },
   "source": [
    "Standard Deviation During Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oIKnG0FHE26A"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.062456872"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std(history.history['dice_coef'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jUa9K2_2zo9g"
   },
   "source": [
    "Evaluate model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ERN9YXmEzu7v"
   },
   "outputs": [],
   "source": [
    "score = model_full.evaluate_generator(test_gen, steps=1, use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Su88aabbuu9n"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing dice loss: 0.022849619388580322\n",
      "Testing dice score: 0.9771503806114197\n"
     ]
    }
   ],
   "source": [
    "print('Testing dice loss:', score[0])\n",
    "print('Testing dice score:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YyZAVmA3FCEP"
   },
   "source": [
    "Confusion Matrix For Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b3wlBWlyFN0m"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqohufmroLaI"
   },
   "source": [
    "## Predict segmentation on few sample images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YPl-IWgsljso"
   },
   "outputs": [],
   "source": [
    "#using Flair and T2 as input for full tumor segmentation\n",
    "x = np.zeros((1,2,240,240),np.float32)\n",
    "x[:,:1,:,:] = Img_Flair[80:81,:,:,:]\n",
    "x[:,1:,:,:] = Img_T2[80:81,:,:,:] \n",
    "\n",
    "pred_full = model_full.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q9s3RJzCncVk"
   },
   "outputs": [],
   "source": [
    "slice_no = 80\n",
    "plt.figure(figsize=(15,10))\n",
    "\n",
    "plt.subplot(141)\n",
    "plt.title('T2')\n",
    "plt.axis('off')\n",
    "plt.imshow(Img_T2[slice_no, 0, :, :],cmap='gray')\n",
    "    \n",
    "plt.subplot(142)\n",
    "plt.title('Flair')\n",
    "plt.axis('off')\n",
    "plt.imshow(Img_Flair[slice_no, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(143)\n",
    "plt.title('Ground Truth(full)')\n",
    "plt.axis('off')\n",
    "plt.imshow(Seg_full[slice_no, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.subplot(144)\n",
    "plt.title('prediction(full)')\n",
    "plt.axis('off')\n",
    "plt.imshow(pred_full[0, 0, :, :],cmap='gray')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "CqMyXJ3JTdEK",
    "_fRq2q_lMAbU",
    "JkxWaejoYazV",
    "G73ur1tVerBO",
    "eaBS9fz5P7UT",
    "b-sifTU2yHN0",
    "kqohufmroLaI"
   ],
   "name": "Part1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
